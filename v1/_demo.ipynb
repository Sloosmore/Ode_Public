{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import assemblyai as aai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "aai.settings.api_key = os.getenv(\"AS_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_def import *\n",
    "from train_script import *\n",
    "from save_load import *\n",
    "from test_inference import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds_json = '../g_sheet_auth.json'\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(creds_json, scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "sheet_url = 'Grind Sheet v2'\n",
    "sheet = client.open(sheet_url).sheet1  \n",
    "df = pd.DataFrame(sheet.get_all_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Group                                                    Control\n",
      "Independent Var                                                    NA\n",
      "Complete                                                         TRUE\n",
      "Data Points                                                     50000\n",
      "Batch Size                                                         64\n",
      "Encoder Dict        {'path':'sentence-transformers/all-MiniLM-L6-v...\n",
      "Same Eno                                                         TRUE\n",
      "Encoder Req Grad                                                FALSE\n",
      "Concat Method                                       (\"u\",\"v\",\"|u-v|\")\n",
      "FFNN                nn.Sequential(    \\n    nn.Linear(in_features=...\n",
      "Epochs                                                             40\n",
      "Freeze Epochs                                                    None\n",
      "lr                                                              0.001\n",
      "Train Loss                                                      0.062\n",
      "Test Loss                                                       0.062\n",
      "Train F1                                                        0.955\n",
      "Test F1                                                          0.97\n",
      "Cost per hour                                                     0.0\n",
      "Time (Hours)                                                 0.603056\n",
      "Date                                                        July 16th\n",
      "Notes                                                                \n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "extract_index = 0\n",
    "save_path = \"../../saved_models/V2/20-ep/20-ep.pt\"\n",
    "\n",
    "hyperparameters = df.iloc[0]\n",
    "print(hyperparameters)\n",
    "encoder_dict = ast.literal_eval(hyperparameters['Encoder Dict'])\n",
    "end_pred_sequential = eval(hyperparameters['FFNN'])\n",
    "same_encoder = True if hyperparameters['Same Eno'].lower() == \"true\" else False\n",
    "encoder_grad = True if hyperparameters['Encoder Req Grad'].lower() == \"true\" else False\n",
    "concat_method = eval(hyperparameters[\"Concat Method\"])\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_dict['path'])\n",
    "encoder = AutoModel.from_pretrained(encoder_dict['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# model = TSN_model(encoder_model=encoder_dict['path'], \n",
    "#                   context_vector_len=encoder_dict['size'], \n",
    "#                   device=device, \n",
    "#                   same_encoder=same_encoder,\n",
    "#                   encoder_tune=encoder_grad,\n",
    "#                   end_pred_sequential=end_pred_sequential,\n",
    "#                   concat_method=concat_method).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "  (1): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=1152, out_features=512, bias=True)\n",
      "  (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "final_unit = nn.Sequential(    \n",
    "    nn.Linear(in_features=3*encoder_dict['size'], out_features=3*encoder_dict['size']),\n",
    "    nn.BatchNorm1d(3*encoder_dict['size']),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=3*encoder_dict['size'], out_features=512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 1)\n",
    ")\n",
    "post_path = \"../../saved_models/Control/FFNN.pt\"\n",
    "post_e_model = load_post_encoder(model=final_unit, path=post_path)\n",
    "print(post_e_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "        \"\"\"\n",
    "        Applies mean pooling to the model output.\n",
    "\n",
    "        Args:\n",
    "            model_output: The output from the model.\n",
    "            attention_mask: The attention mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The pooled output.\n",
    "        \"\"\"\n",
    "        #token_embeddings = model_output[0] First element of model_output contained all token embeddings grabed in embed_reshape\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(model_output.size()).float()\n",
    "        return torch.sum(model_output * input_mask_expanded, -2) / torch.clamp(input_mask_expanded.sum(-2), min=1e-9)  \n",
    "      \n",
    "def embed_str(string:str):\n",
    "    tok = tokenizer(string, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoding = encoder(**tok)\n",
    "    return mean_pooling(encoding[0], tok['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_thres = embed_str(threshold)\n",
    "encode_test = embed_str(test_caption)\n",
    "\n",
    "def compute_bool(model, encode_cap, encode_thres, threshold:int=.5):\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        input = torch.cat((encode_cap, encode_thres, torch.abs(encode_cap-encode_thres)), dim=-1)\n",
    "        logit = model(input)\n",
    "        prob = torch.sigmoid(logit).item()\n",
    "        return True if prob > threshold else False\n",
    "\n",
    "compute_bool(post_e_model, encode_test, encode_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import yagmail\n",
    "\n",
    "def send_message():\n",
    "    \n",
    "    sender_email = \"stanloosmore@gmail.com\"\n",
    "    app_password = \"ebfegwqbvuyosilm\"\n",
    "\n",
    "    try:\n",
    "        # initializing the server connection\n",
    "        yag = yagmail.SMTP(user=sender_email, password=app_password)\n",
    "\n",
    "        # sending the email\n",
    "        yag.send(to=\"loosmore@usc.edu\", \n",
    "                subject=\"Demo fired1\", \n",
    "                contents=\"This is my website: withode.com\")\n",
    "\n",
    "        print(\"Email sent successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # applescript = f'''\n",
    "    # tell application \"Messages\"\n",
    "    #     set targetService to 1st service whose service type = iMessage\n",
    "    #     set targetBuddy to buddy \"{phone_number}\" of targetService\n",
    "    #     send \"{message}\" to targetBuddy\n",
    "    # end tell\n",
    "    # '''\n",
    "    \n",
    "    # subprocess.run([\"osascript\", \"-e\", applescript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = \"agree to email a website\"\n",
    "test_caption = \"let me do the demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phone_number = \"2069799210\"\n",
    "# baylor_num = \"4159994536\"\n",
    "# message='withode.com'\n",
    "triggered = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_data(transcript: aai.RealtimeTranscript):\n",
    "  \"This function is called when a new transcript has been received.\"\n",
    "  global triggered\n",
    "\n",
    "  if not transcript.text:\n",
    "    return\n",
    "\n",
    "  if isinstance(transcript, aai.RealtimeFinalTranscript):\n",
    "    caption = transcript.text\n",
    "    print(caption, end=\"\\r\\n\")\n",
    "    encode_cap = embed_str(caption)\n",
    "    is_satisfied = compute_bool(post_e_model, encode_cap, encode_thres)\n",
    "    if is_satisfied and not triggered:\n",
    "      print(\"SENDING MESSAGE\", end=\"\\r\\n\")\n",
    "      send_message()    # run it throught the pipeline\n",
    "      triggered = True\n",
    "    else:\n",
    "      print('not met')\n",
    "  else:\n",
    "    print(transcript.text, end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_error(error: aai.RealtimeError):\n",
    "  print(\"An error occured:\", error)\n",
    "\n",
    "def on_open(session_opened: aai.RealtimeSessionOpened):\n",
    "  \"This function is called when the connection has been established.\"\n",
    "\n",
    "  print(\"Session ID:\", session_opened.session_id)\n",
    "\n",
    "transcriber = aai.RealtimeTranscriber(\n",
    "  on_data=on_data,\n",
    "  on_error=on_error,\n",
    "  sample_rate=44_100,\n",
    "  on_open=on_open, # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: dfbebe1f-0055-4064-bcf8-0724b4121b75\n",
      "Hey, I'm Stan, and I'll be giving a brief demo.\n",
      "not met\n",
      "So if you remember, my whole thing was around.\n",
      "not met\n",
      "Threshold detection and using natural language.\n",
      "not met\n",
      "To figure out.\n",
      "not met\n",
      "If something was met or something was inside of another thing.\n",
      "not met\n",
      "So now I'll actually give you an example of that.\n",
      "not met\n",
      "And so let's say we're in a conversation.\n",
      "not met\n",
      "And you were like, hey, Stan, I'm really curious about what you've been working on.\n",
      "not met\n",
      "Can you give me some more information?\n",
      "not met\n",
      "And I would say.\n",
      "not met\n",
      "Great. Let me send you my website.\n",
      "SENDING MESSAGE\n",
      "Email sent successfully\n",
      "And then it would be sent to you.\n",
      "not met\n",
      "Like, how cool would that be? Well.\n",
      "not met\n",
      "I just haven't. So let me show you what this means.\n",
      "not met\n",
      "If we go into here. 657. 657.seven six fifty seven\n",
      "not met\n",
      "So I'll tell you. I hard coded it to send you a website.\n",
      "not met\n",
      "Because that's not what I'm working on. What I'm working on specifically. Is the natural language part that detects if there is a threshold. So you're probably wondering. Okay, but you didn't even show what the threshold was. Well, why don't. I. So if we come up here.\n",
      "not met\n",
      "Agree to email website. Now you can get specific or as broad like just agree to send someone. Information, it would work. Agree?\n",
      "not met\n",
      "To.\n",
      "not met\n",
      "You'll notice we can get more specific, right?\n",
      "not met\n",
      "Now. I could have said something else.\n",
      "not met\n",
      "But I'll be giving you the model.\n",
      "not met\n",
      "So feel free to try it out.\n",
      "not met\n",
      "And this is the worst it's ever going to be. So that's the demo.\n",
      "not met\n"
     ]
    }
   ],
   "source": [
    "transcriber.connect()\n",
    "# Open a microphone stream\n",
    "microphone_stream = aai.extras.MicrophoneStream()\n",
    "\n",
    "# Press CTRL+C to abort\n",
    "transcriber.stream(microphone_stream)\n",
    "\n",
    "transcriber.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
